\documentclass[11pt, twoside, letter]{article}
\usepackage[margin = 1in]{geometry}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts,bm}
\usepackage{ifthen}
\usepackage[boxed, linesnumbered]{algorithm2e}
%\usepackage{comment}
%\usepackage[unicode,psdextra,colorlinks=true,citecolor=blue,bookmarksnumbered=true,linkcolor=green!40!black]{hyperref}
\usepackage{hyperref}
%Figures
\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}


\usepackage{microtype}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

%%%%Referencing
%Cleveref must be loaded last
%amsthm must be after amsmath
\usepackage{amsthm}
%\usepackage[bookmarks=false]{hyperref}
\usepackage{multirow, multicol}
\pagestyle{plain}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{observation}[theorem]{Observation}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{example}[theorem]{Example}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{question}{Question}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{definition}[theorem]{Definition}
%\newcommand*{\email}[1]{\texttt{#1}}


\numberwithin{equation}{section}

%    Absolute value notation
\newcommand{\abs}[1]{\lvert#1\rvert}
\renewcommand{\vec}[1]{\bm{#1}}

%    Blank box placeholder for figures (to avoid requiring any
%    particular graphics capabilities for printing this document).
\newcommand{\blankbox}[2]{%
  \parbox{\columnwidth}{\centering
%    Set fboxsep to 0 so that the actual size of the box will match the
%    given measurements more closely.
    \setlength{\fboxsep}{0pt}%
    \fbox{\raisebox{0pt}[#2]{\hspace{#1}}}%
  }%
}
% \documentclass[12pt]{colt2019} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\usepackage{times}
\usepackage{comment}

\begin{document}


\title{Empirical Analysis of Gradient EM for Truncated Mixture of Gaussians}

%\author{Sai Ganesh Nagarajan\\SUTD\\sai\_nagarajan@mymail.sutd.edu.sg
%\and Ioannis Panageas\\SUTD\\ioannis@sutd.edu.sg}

% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}



% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
\date{}
%\keywords%
%  {Expectation-Maximization, Mixture of two Gaussians, Convergence, Stability}
\maketitle

\begin{abstract}
	Abstract here...
\end{abstract}

\section{Introduction}

\input{intro}
\input{Background}
%\input{EM-algo}
\input{Experiments}
%\input{Sym-truncation}
%\input{Arb-truncation}
%\input{more_fixed_points}
%\input{rates}

%\section{Conclusion}
%In this paper, we studied the convergence properties of EM applied to the problem of truncated mixture of two Gaussians. We managed to show that EM converges almost surely to the true mean in the case $d=1$ (with an exponential rate depending on $\alpha$) and moreover that the same result carries over for $d>1$ under the assumption that the update rule of EM has only three fixed points (if it has more, then our results imply local convergence of EM if the initializations are close enough to the true mean). Some interesting questions that arise from this line of work are the following:
%\begin{itemize}
%	%\item Rate of convergence: Our result is qualitative. An interesting open direction is to find bounds on the rate of convergence.
%	\item Finite population case: Our setting assumes infinite samples. Can we prove a similar convergence result using only finitely many samples? The multi-dimensional case will be challenging because of the existence of more than three fixed points in general.
%	\item Beyond two components: Characterize the truncated sets $S$ for which EM converges almost surely to the true mean for truncated mixture of $k$-Gaussians, where $k \geq 3$.
%\end{itemize}


% Acknowledgments---Will not appear in anonymized version
%\section*{Acknowledgements}
%{IP would like to thank Arnab Bhattacharya and Costis Daskalakis for fruitful discussions.}

\bibliographystyle{plain}
\bibliography{colt2019}

\appendix
%\input{appendix}

\begin{comment}
\section{Proof of Lemma 1}
Let us denote $c_1$ for the component of the Gaussian corresponding to $+\vec{\lambda}$ and $c_2$ denote the component corresponding to $-\vec{\lambda}$.
First we have to find the posterior densities in the Expectation step. Let $\vec{\lambda}_t$ be our estimate of the parameter at time $t$.

\begin{align}
	\begin{split}
		Q_{\vec{\lambda}_t}(c_1)&=\PP_{\vec{\lambda}_t,S}(Z=c_1|\vec{X}=\vec{x})\\
		&=\frac{\PP_{\vec{\lambda}_t,S}(\vec{X}=\vec{x}|Z=c_1)\PP(Z=c_1)}{\PP_{\vec{\lambda}_t,S}(\vec{X}=\vec{x})}\\
		&=\frac{\phi(\vec{x};\vec{\lambda}_t,\vec{\Sigma})}{\phi(\vec{x};\vec{\lambda}_t,\vec{\Sigma})+\phi(\vec{x};-\vec{\lambda}_t,\vec{\Sigma})}\\
		Q_{\vec{\lambda}_t}(c_2)&=\frac{\phi(\vec{x};-\vec{\lambda}_t,\vec{\Sigma})}{\phi(\vec{x};\vec{\lambda}_t,\vec{\Sigma})+\phi(\vec{x};-\vec{\lambda}_t,\vec{\Sigma})}
	\end{split}
\end{align}

Now the maximization step involves the following:
\begin{align}
	\vec{\lambda}_{t+1}&=\argmax_{\vec{\lambda}}\left[Q_{\vec{\lambda}_t}(c_1)\log\frac{\PP_{\vec{\lambda},S}(\vec{x},c_1)}{Q_{\vec{\lambda}_t}(c_1)}+Q_{\vec{\lambda}_t}(c_2)\log\frac{\PP_{\vec{\lambda},S}(\vec{x},c_2)}{Q_{\vec{\lambda}_t}(c_2)}\right]
\end{align}

Now substituting for $Q_{\vec{\lambda}_t}(c_1)$, $Q_{\vec{\lambda}_t}(c_2)$ and writing $\PP_{\vec{\lambda},S}(\vec{x},c_1)=\frac{\phi(\vec{x};\vec{\lambda},\vec{\Sigma})}{\int_S\phi(\vec{x};\vec{\lambda},\vec{\Sigma})+\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})d\vec{x}}$, similarly $\PP_{\vec{\lambda},S}(\vec{x},c_2)=\frac{\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})}{\int_S\phi(\vec{x};\vec{\lambda},\vec{\Sigma})+\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})d\vec{x}}$, we get the following:

\begin{align}
	\begin{split}
		\vec{\lambda}_{t+1}&=\argmax_{\vec{\lambda}}\left[Q_{\vec{\lambda}_t}(c_1)\log\frac{\phi(\vec{x};\vec{\lambda},\vec{\Sigma})}{\int_S\phi(\vec{x};\vec{\lambda},\vec{\Sigma})+\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})d\vec{x}}+Q_{\vec{\lambda}_t}(c_2)\log\frac{\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})}{\int_S\phi(\vec{x};\vec{\lambda},\vec{\Sigma})+\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})d\vec{x}}\right]\\
		&=\argmax_{\vec{\lambda}}\bigg[Q_{\vec{\lambda}_t}(c_1)\big(\log\phi(\vec{x};\vec{\lambda},\vec{\Sigma})-\log\int_S\phi(\vec{x};\vec{\lambda},\vec{\Sigma})+\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})d\vec{x}\big)\\
		&+Q_{\vec{\lambda}_t}(c_2)\big(\log\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})-\log\int_S\phi(\vec{x};\vec{\lambda},\vec{\Sigma})+\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})d\vec{x}\big)\bigg]\\
		&=\argmax_{\vec{\lambda}}\bigg[Q_{\vec{\lambda}_t}(c_1)\big(\log\phi(\vec{x};\vec{\lambda},\vec{\Sigma})-\log\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})\big)\\
		&+\log\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})-\log\int_S\phi(\vec{x};\vec{\lambda},\vec{\Sigma})+\phi(\vec{x};-\vec{\lambda},\vec{\Sigma})d\vec{x}\bigg]\\
	\end{split}
\end{align}

Finding the gradient of the above maximization we get the following:

\begin{align}
	\begin{split}
		\nabla_{\vec{\lambda}}g(\vec{\lambda};\vec{x},\vec{\Sigma})&=\frac{d}{d\vec{\lambda}}\bigg[Q_{\vec{\lambda}_t}(c_1)\left(2\vec{x}^T\vec{\Sigma}^{-1}\vec{\lambda}\right)-0.5*\vec{x}^T\vec{\Sigma}^{-1}\vec{x}-0.5*\vec{\lambda}^T\vec{\Sigma}^{-1}\vec{\lambda}-\vec{x}^T\vec{\Sigma}^{-1}\vec{\lambda}\\
		&-\log\int_S 2*f_{\vec{\lambda}}(\vec{x}) d\vec{x}\bigg]\\
		&=\left(2Q_{\vec{\lambda}_t}(c_1)-1\right)\vec{x}^T\vec{\Sigma}^{-1}-\vec{\lambda}^{T}\vec{\Sigma}^{-1}-\int\frac{d}{d\vec{\lambda}}f_{\vec{\lambda},S}(\vec{x}) d\vec{x}\\
		&=\left(2Q_{\vec{\lambda}_t}(c_1)-1\right)\vec{x}^T\vec{\Sigma}^{-1}-\vec{\lambda}^{T}\vec{\Sigma}^{-1}-\int\frac{d}{d\vec{\lambda}}\log f_{\vec{\lambda}}(\vec{x}) f_{\vec{\lambda},S}(\vec{x}) d\vec{x}\\
		&=\left(2Q_{\vec{\lambda}_t}(c_1)-1\right)\vec{x}^T\vec{\Sigma}^{-1}-\vec{\lambda}^{T}\vec{\Sigma}^{-1}-\EE_{\lambda,S}\left[-\vec{\lambda}^T\vec{\Sigma}^{-1}+\vec{x}^T\vec{\Sigma}^{-1}\tanh(\vec{x}^T\vec{\Sigma}^{-1}\vec{\lambda})\right]\\
		&=\left(2Q_{\vec{\lambda}_t}(c_1)-1\right)\vec{x}^T\vec{\Sigma}^{-1}-\EE_{\lambda,S}\left[\vec{x}^T\vec{\Sigma}^{-1}\tanh(\vec{x}^T\vec{\Sigma}^{-1}\vec{\lambda})\right]\\
		&=\tanh(\vec{x}^T\vec{\Sigma}^{-1}\vec{\lambda}_t)\vec{x}^T\vec{\Sigma}^{-1}-\EE_{\lambda,S}\left[\vec{x}^T\vec{\Sigma}^{-1}\tanh(\vec{x}^T\vec{\Sigma}^{-1}\vec{\lambda})\right]
	\end{split}
\end{align}

Thus under the infinite sample case we have the following EM update rule:

\begin{align}
	\vec{\lambda}_{t+1}=\left\{\vec{\lambda}:h(\vec{\lambda}_t,\vec{\lambda})=0\right\}
\end{align}
such that $h(\vec{\lambda}_t,\vec{\lambda}_{t+1})=0$.

where,
\begin{align}
	h(\vec{\lambda}_t,\vec{\lambda}):=\EE_{\vec{\mu},S}\left[\tanh(\vec{x}^T\vec{\Sigma}^{-1}\vec{\lambda}_t)\vec{x}^T\vec{\Sigma}^{-1}\right]-\EE_{\vec{\lambda},S}\left[\vec{x}^T\vec{\Sigma}^{-1}\tanh(\vec{x}^T\vec{\Sigma}^{-1}\vec{\lambda})\right]
\end{align}

\section{My Proof of Theorem 2}

This is a complete version of a proof sketched in the main text.
\end{comment}

\end{document}
