Expectation-Maximization (EM) is an iterative algorithm, widely used to compute the maximum likelihood estimation of parameters in statistical models that depend on hidden (latent) variables $\vec{z}$ (\cite{DLR77}). Given a
probability distribution $p_{\vec{\lambda}}$ defined on $(\vec{x};\vec{z})$, where $\vec{x}$ are the observables and $\vec{\lambda}$ is a parameter vector, and samples $\vec{x}_1,...,\vec{x}_n$, one wants to find $\vec{\lambda}$ in order to maximize the log-likelihood $\sum_{i=1}^n \log p_{\vec{\lambda}}(\vec{x})$ (finite sample case) or $\mathbb{E}_{\vec{x}}[\log p_{\vec{\lambda}}(\vec{x})]$ (population version). Such a task is not always easy, because computing the log-likelihood involves summations over all the possible values of the latent variables and moreover the log-likelihood might be non-concave. EM algorithm is one way to tackle the described problem and works as follows:
\begin{itemize}
\item Guess an initialization of the parameters $\vec{\lambda}_0$.
\item For each iteration:\\ (Expectation-step) Compute the posterior $Q_i(\vec{z})$, which is $p_{\vec{\lambda}_t}(\vec{z} | \vec{x}_i)$ for each sample $i$.\\ (Maximization-step) Compute $\vec{\lambda}_{t+1}$ as the argmax of $\sum_i \sum_{\vec{z}} Q_i (\vec{z}) \log \frac{p_{\vec{\lambda}}(\vec{z},\vec{x})}{Q_i (\vec{z})}$.
\end{itemize}

It is well known that there are guarantees for convergence of EM to stationary points \cite{W83}. The idea behind this fact is that the log-likelihood is decreasing along the trajectories of the EM dynamics. We note that this is true for the case of the truncated mixture of Gaussians too. One of its main applications is on learning mixture of Gaussians. Recovering the parameters of a mixture of Gaussians with strong guarantees was initiated by Dasgupta \cite{D99} and has been extensively studied in theoretical computer science and machine learning communities, e.g., \cite{AK01}, \cite{KSV05}, \cite{CR08}, \cite{CDV09}, where most of the works assume that the means are well-separated. In addition, the authors in \cite{MV10}, \cite{KMV10} offer stronger guarantees, polynomial time (in dimension $d$) learnability of Gaussian mixtures.

Recent results indicate that EM works well (converges to true mean) for mixture of two Gaussians (see \cite{XHM16}, \cite{DTZ17} for global convergence and \cite{BWY15} for local convergence), a result that is not true if the number of components is at least three (in \cite{JZBWJ16} an example is constructed where the log-likelihood landscape has local maxima that are not global and EM converges to these points with positive probability).

Parameter estimation problems involving data that has been censored/truncated is crucial in many statistical problems occurring in practice. Statisticians, dating back to Pearson \cite{PA08} and Fisher \cite{F31}, tried to address this problem in the early 1900's. Techniques such as method of moments and maximum-likelihood were used for estimating a Gaussian distribution from truncated samples. The seminal work of Rubin \cite{R76} in 1976, on missing/censored data, tried to approach this by a framework of ignorable and non-ignorable missingness, where the reason for missingness is incorporated into the statistical model through. However, in many cases such flexibilities may not be available. More recent work on estimation with truncated data has focused on tractable parametric models such as Gaussians, thereby providing strong computational guarantees for convergence to the true parameters \cite{DGTZ18}.
Mixture models are ubiquitous in machine learning and statistics with a variety of applications ranging from biology \cite{BF08,APMP07} to finance \cite{BM02}. Many of these practical applications are not devoid of some form of truncation or censoring. To this end, there has been previous work that uses EM algorithm for Gaussian mixtures in this setting \cite{LS12}, \cite{MJ88}. However, they assume truncation sets that are generally boxes and in addition do not provide any convergence guarantees.

\paragraph{Our results and techniques}

Our results can be summarized in the following two theorems (one for single-dimensional and one for multi-dimensional case):

\begin{theorem}[Single-dimensional case]\label{thm:single} Let $S \subset \mathbb{R}$ be an arbitrary measurable set of positive Lebesgue measure, i.e, $\int_S \mathcal{N}(x;\mu, \sigma^2) + \mathcal{N}(x;-\mu, \sigma^2) dx = \alpha>0$. It holds that under random initialization (under a measure on $\mathbb{R}$ that is absolutely continuous w.r.t Lebesgue), EM algorithm converges with probability one to either $\mu$ or $-\mu$. Moreover, if initialization $\lambda_0>0$ then EM converges to $\mu$ with an exponential rate \[|\vec{\lambda}_{t+1}- \mu| \leq \rho_t |\lambda_t - \mu|,\] with $\rho_t = 1 - \Omega(\alpha^4)\min(\alpha^2\min(\lambda_t,\mu),1)$ which is decreasing in $t$. Analogously if $\lambda_0 < 0$, it converges to $-\mu$ with same rate (substitute $\max(\lambda_t,-\mu)$ in the expression).
\end{theorem}

\begin{theorem}[Multi-dimensional case]\label{thm:multi} Let $S \subset \mathbb{R}^d$ with $d>1$ be an arbitrary measurable set of positive Lebesgue measure so that $\int_S \mathcal{N}(\vec{x};\vec{\mu}, \vec{\Sigma}) + \mathcal{N}(\vec{x};\vec{-\mu}, \vec{\Sigma}) d\vec{x} = \alpha>0$. It holds that under random initialization (according to a measure on $\mathbb{R}^d$ that is absolutely continuous with Lebesgue measure), EM algorithm converges with probability one to either $\vec{\mu}$ or $\vec{-\mu}$ as long as EM update rule has only $\vec{-\mu},\vec{0},\vec{\mu}$ as fixed points. Moreover, if $\lambda_0$ is in a neighborhood of $\vec{\mu}$ or $-\vec{\mu}$, it converges with a rate $1 - \Omega(\alpha^6)$\footnote{If $ \alpha \mu \ll 1$ then the global convergence rate we provide in the single-dimensional case coincides with the local convergence rate of multidimensional.}.
\end{theorem}

\begin{remark} We would like first to note that we prove the two theorems above in a more general setting where we have truncation functions instead of truncation sets (see Section \ref{sec:model} for definitions). Furthermore, in the proof of Theorem \ref{thm:multi}, we show that $\vec{0}$ is a repelling fixed point and moreover $\vec{-\mu},\vec{\mu}$ are attracting so if the initialization is close enough to $\vec{-\mu}$ or $\vec{\mu}$, then EM actually converges to the true mean. Finally, in Section \ref{sec:multi}, Lemma \ref{lem:symrotation} we provide sufficient conditions of the truncated set $S$ (or truncation function) so that the EM update rule has exactly three fixed points. The sufficient condition is that $S$ is rotation invariant under some appropriate transformation.
\end{remark}

To prove the qualitative part of our two main theorems, we perform stability analysis on the fixed points $\vec{-\mu},\vec{0},\vec{\mu}$ of the dynamical system that is induced by EM algorithm and moreover show that the update rule is a diffeomorphism. This is a general approach that has appeared in other papers that talk about first-order methods avoiding saddle points (\cite{MPP15}, \cite{LSJR16}, \cite{PP17}, \cite{LPPSJR17}, \cite{DP18} to name a few).

Nevertheless, computing the update rule of EM for a truncated mixture of two Gaussians is not always possible, because the set/function $S$ is not necessarily symmetric around $\vec{0}$ (even for functions). As a result, the techniques of \cite{DTZ17} (for the population version) do not carry over to our case. In particular we can find an \textit{implicit} description of the update rule of the EM.

Moreover, getting inspiration from the \textit{Implicit Function Theorem}, we are able to compute explicitly the Jacobian of the update rule of EM and perform spectral analysis on it (Jacobian is computed at the three fixed points $\vec{-\mu},\vec{0}, \vec{\mu}$). We show that the spectral radius of the Jacobian computed at $\vec{-\mu},\vec{\mu}$ is less than one (the fixed points are attracting locally) and moreover the spectral radius of the Jacobian computed at $\vec{0}$ is greater than one (repelling). Along with the fact that the Jacobian is invertible (hence the update rule of EM is a diffeomorphism\footnote{A function is called a diffeomorphism if it is differentiable and a bijection and its inverse is differentiable.}), we can use the center-stable manifold theorem to show that the region of attraction of fixed point $\vec{0}$ is of measure zero. Due to the fact that EM converges always to stationary points (folklore), our result follows. We note that in the case $d=1$, the fixed points are exactly three ($-\mu,0,\mu$) and we prove this fact using FKG (see Theorem \ref{thm:FKG}) inequality. As far as the case $d>1$ is concerned, if $S$ is rotation invariant (under proper transformation so that covariance matrix becomes identity), we can show that there are exactly three fixed points by reducing it to the single dimensional case. Last but not least, for the rates of convergence (quantitative part of our theorems), we prove a quantitative version of the FKG inequality (see Lemma \ref{lem:FKGrevise}) which also might be of independent interest. Due to space constraints, please see supplementary material for the proofs.
